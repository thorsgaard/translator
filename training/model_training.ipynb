{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is for model training, and can be used to train or recalibrate a new model.\n",
    "https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.persistence.adapter.data_cleaning_adapter import EngDaDataCleaningAdapter\n",
    "from src.domain.services.encoding_service import EncodingService"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#clean data\n",
    "#Encode tokens\n",
    "#train model\n",
    "#run application with trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Read data adn clean\n",
    "data = EngDaDataCleaningAdapter().get_clean_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(data, test_size=0.2, random_state = 12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It’s time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. This has to be done for both the train and test datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "da_index = 0\n",
    "en_index = 1\n",
    "\n",
    "da_length = len(data[:, da_index])\n",
    "en_length = len(data[:, en_index])\n",
    "\n",
    "da_encoder = EncodingService(data[:,da_index])\n",
    "en_encoder = EncodingService(data[:,en_index])\n",
    "\n",
    "da_vocab_size = len(da_encoder.tokenizer.word_index)+1\n",
    "en_vocab_size = len(en_encoder.tokenizer.word_index)+1\n",
    "\n",
    "# Encode training data\n",
    "trainX = da_encoder.encode_tokens(da_length, train[:, da_index])\n",
    "trainY = en_encoder.encode_tokens(en_length, train[:, en_index])\n",
    "\n",
    "# Encode validation data\n",
    "testX = da_encoder.encode_tokens(da_length, test[:, da_index])\n",
    "testY = en_encoder.encode_tokens(en_length, test[:, en_index])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Delete object to clear up memory for model training\n",
    "#Need to wait until garbage collection after deletion of objects\n",
    "del train\n",
    "del da_encoder\n",
    "del en_encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "\n",
    "# build NMT model\n",
    "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "      model.add(LSTM(units))\n",
    "      model.add(RepeatVector(out_timesteps))\n",
    "      model.add(LSTM(units, return_sequences=True))\n",
    "      model.add(Dense(out_vocab, activation='softmax'))\n",
    "      return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using the RMSprop optimizer in this model as it’s usually a good choice when working with recurrent neural networks.\n",
    "\n",
    "We will train it for 30 epochs and with a batch size of 512 with a validation split of 20%. 80% of the data will be used for training the model and the rest for evaluating it. You may change and play around with these hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# model compilation\n",
    "model = define_model(da_vocab_size, en_vocab_size, da_length, en_length, 512)\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "DIR = \"../src/persistence/models/\"\n",
    "filename = 'en_da_model.v1'\n",
    "\n",
    "checkpoint = ModelCheckpoint((DIR + filename),\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "# train model\n",
    "history = model.fit(trainX,\n",
    "                    trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "5492"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Text\n",
    "- Tokens, which is a sequence of character, and a subpart of original text\n",
    "- embeddings, is encoded tokens into integer arrays\n",
    "- transformers, which is adding self attention to the embeddings\n",
    "\n",
    "We now have a model, that with a given input can predict a sequence of tokens having the highest probability to meet the desired outcome."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
